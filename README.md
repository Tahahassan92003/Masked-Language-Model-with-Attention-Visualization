# Masked-Language-Model-with-Attention-Visualization

# 🔍 BERT Masked Token Prediction & Attention Visualization

This project uses **BERT (bert-base-uncased)** to predict missing words in a sentence and visualize **attention mechanisms** using heatmaps.

## 🚀 Features
- 📖 **Masked Word Prediction** – Predicts the most likely words for `[MASK]` in a sentence.
- 🎨 **Attention Visualization** – Generates heatmaps to show how tokens attend to each other.
- ⚡ **Pre-trained Model** – Uses Hugging Face's `transformers` library.
- 🖼 **Custom Font Support** – Renders attention diagrams using `PIL`.

## 📦 Installation
Ensure Python is installed, then install dependencies:

```sh
pip install tensorflow transformers pillow

