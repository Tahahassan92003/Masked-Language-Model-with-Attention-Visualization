# Masked-Language-Model-with-Attention-Visualization

# ğŸ” BERT Masked Token Prediction & Attention Visualization

This project uses **BERT (bert-base-uncased)** to predict missing words in a sentence and visualize **attention mechanisms** using heatmaps.

## ğŸš€ Features
- ğŸ“– **Masked Word Prediction** â€“ Predicts the most likely words for `[MASK]` in a sentence.
- ğŸ¨ **Attention Visualization** â€“ Generates heatmaps to show how tokens attend to each other.
- âš¡ **Pre-trained Model** â€“ Uses Hugging Face's `transformers` library.
- ğŸ–¼ **Custom Font Support** â€“ Renders attention diagrams using `PIL`.

## ğŸ“¦ Installation
Ensure Python is installed, then install dependencies:

```sh
pip install tensorflow transformers pillow

